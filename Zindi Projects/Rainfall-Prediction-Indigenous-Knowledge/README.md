# Rainfall Prediction Using Indigenous Knowledge

This project builds an XGBoost classifier to predict rainfall intensity  
(**No Rain**, **Small Rain**, **Medium Rain**, **Heavy Rain**) using  
features derived from Indigenous Knowledge (IK) alongside basic numerical variables.

The main objective is to evaluate how useful community-based environmental indicators â€”
such as plant cues, animal behavior, and local observations â€” are when combined with
simple numerical/contextual features.

---

## ğŸŒ Dataset

After initial cleaning, the dataset includes the following usable columns:

### **ğŸ”¥ One-hot encoded categorical columns**
- `community`
- `district`
- `indicator`
- `pred_date`

### **ğŸ”¢ Numeric/continuous columns used as-is**
- `user_id`
- `predicted_intensity`
- `confidence`
- `forecast_length`
- `month`
- `day`
- `hour`
- `dow`

Columns such as **`time_observed`** and **`indicator_description`** were dropped during preprocessing.

### ğŸ·ï¸ Target variable
- `rainfall_level`  
  (four classes: No Rain, Small Rain, Medium Rain, Heavy Rain)

### âš ï¸ Note on Imbalance  
The dataset is imbalanced, but *no* class-weighting, oversampling, or SMOTE was applied.
The model is trained directly on the raw class distribution.

---

## ğŸ§  Modeling Approach (XGBoost Only)

This project intentionally focuses on **one algorithm**:  
â¡ï¸ **XGBoost (XGBClassifier)** for multi-class prediction.

### **Preprocessing Steps**
1. Split dataset into train/test with stratification.
2. Apply **one-hot encoding** on the listed categorical columns.
3. Concatenate with numeric features.
4. Train XGBoost using default or lightly tuned hyperparameters.
5. Evaluate using accuracy, macro F1, and confusion matrix.

No scaling was applied, as XGBoost does not require feature normalization.

---

## ğŸ“ˆ Evaluation

Key evaluation steps include:

- Stratified train/test split  
- Macro F1 to evaluate minority rainfall classes  
- Confusion matrix per class  
- Feature importance to understand which IK indicators contribute the most

Results vary by dataset version and cleaning steps, and will be updated as the project progresses.

---

## ğŸ“ Project Structure

```markdown

rainfall-ik/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01-preprocessing.ipynb
â”‚   â”œâ”€â”€ 02-training-xgboost.ipynb
â”‚   â””â”€â”€ 03-evaluation.ipynb
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocess.py
â”‚   â”œâ”€â”€ train_xgb.py
â”‚   â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ data/          # (optional; usually excluded via .gitignore)
â”œâ”€â”€ models/
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```


---

## ğŸš€ How to Run

1. Clone the repo  
   ```bash
   git clone https://github.com/<your-username>/rainfall-ik.git
   cd rainfall-ik
````

2. Install dependencies

   ```bash
   pip install -r requirements.txt
   ```

3. Open the notebooks

   ```bash
   jupyter notebook
   ```

---

## ğŸ”® Future Improvements

* Experiment with class-balancing strategies
* Compare XGBoost with LightGBM or CatBoost
* Introduce temporal features (lags, trends)
* Perform SHAP analysis for interpretability
* Build a simple inference script or web dashboard

---

## ğŸ“œ License

MIT License.

```

---

If you want, I can also generate:

âœ… `requirements.txt`  
âœ… `.gitignore` for a clean ML project  
âœ… The projectâ€™s GitHub description text  
Just tell me!
```
